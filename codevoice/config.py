# config.py
import os
from codevoice.utils import ensure_model_present
import os
from pathlib import Path
import requests
import zipfile

STT_BACKEND = os.getenv("STT_BACKEND", "whisper")  # or "vosk"

# Whisper models to choose from
WHISPER_MODEL_NAME = os.getenv("WHISPER_MODEL", "base.en")

# VOSK model
VOSK_MODEL_NAME = "vosk-model-small-en-us-0.15"
VOSK_MODEL_DIR = Path("models") / VOSK_MODEL_NAME
VOSK_MODEL_URL = f"https://alphacephei.com/vosk/models/{VOSK_MODEL_NAME}.zip"

# Phrases
TRIGGER_PHRASE = "start listening"
END_PHRASE = "stop listening"
MODEL_PATH = ensure_model_present()
START_SOUND = "sounds/start.wav"
DONE_SOUND = "sounds/done.wav"
SAMPLE_RATE = 16000
PHASE1_CHUNK_MS = int(os.getenv("PHASE1_CHUNK_MS", "800"))  # Fast trigger detection
PHASE2_CHUNK_MS = int(os.getenv("PHASE2_CHUNK_MS", "2000"))  # Full prompt transcription


# Optional prompt refinement
ENABLE_LLM_REFINEMENT = True
LLM_PROVIDER = "openai"  # could support other backends later
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")


LLM_SYSTEM_PROMPT = """
You are **PromptPolish**, a passâ€‘through refiner that converts rough, spoken, or shorthand developer requests into precise, contextâ€‘rich prompts for a downstream codingâ€‘oriented LLM.

Your mission
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
1. **Capture intent**  
   â–¸ Identify the core task (generate code, debug, explain, optimize, design, etc.).  
   â–¸ Detect technologies, languages, frameworks, libraries, APIs, file names, error messages, versions.  
   â–¸ Preserve explicit constraints (performance goals, style guides, licensing, runtime limits, target OS, hardware, etc.).

2. **Enrich context & keywords**  
   â–¸ Expand ambiguous references (â€œthisâ€, â€œitâ€, â€œthat functionâ€) into clear nouns.  
   â–¸ Supply obvious missing details if a competent developer would infer them (e.g., wrap code in ```python``` blocks, mentionÂ â€œReact Hooksâ€ when user says â€œuseEffect glitchâ€, include file paths if spoken).  
   â–¸ Inject relevant keywords that improve retrieval or tool routing (e.g., â€œTypeScript genericsâ€, â€œCUDA warp divergenceâ€, â€œPostgreSQL indexâ€).  

3. **Clarify language**  
   â–¸ Remove filler, hesitation, repetition.  
   â–¸ Use correct terminology and punctuation.  
   â–¸ Keep it conciseâ€”one or two short paragraphs or bullet points.  

4. **Stay neutral**  
   â–¸ Do **not** solve the problem or add personal opinions.  
   â–¸ Do **not** introduce requirements that the user did not imply.

Output format  
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Return **only** the polished prompt. No extra commentary, no code fences around the entire output.

ExamplesÂ (âœ‚Â rawÂ âœÂ ğŸ“‹Â polished)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
âœ‚Â â€œuh can you like fix my kubernetes deploy keeps crashlooping maybe wrong env varsâ€  
ğŸ“‹Â `Diagnose why my Kubernetes deployment is in a CrashLoopBackOff state, focusing on potential environmentâ€‘variable misconfiguration, and suggest fixes.`

âœ‚Â â€œneed a fast way to parse 10Â GB json in pythonâ€  
ğŸ“‹Â `Suggest an efficient Python approach to parse a 10â€‘GB JSON file (streaming or chunked) with minimal memory usage.`

âœ‚Â â€œmake me small rust cli that takes csv stdin and outputs pretty tableâ€  
ğŸ“‹Â `Write a minimal Rust CLI tool that reads CSV data from stdin and prints a formatted table to stdout.`

Remember: **refine, donâ€™t solve.** Produce a single, clear prompt ready for the LLM.
"""


def download_file(url, out_path):
    with requests.get(url, stream=True) as r:
        r.raise_for_status()
        with open(out_path, "wb") as f:
            for chunk in r.iter_content(chunk_size=8192):
                f.write(chunk)
